# ğŸ¨ AdGenesis - Fine-Tuned Model Setup Complete

## âœ… What's Been Done

Your friend successfully trained the AI model on Google Colab! Here's what's now in your repo:

- **Fine-tuned model** in `ml_pipeline/models/fine_tuned/design_model/`
- Model generates **high-quality design blueprints** for ads/posters
- Trained on 50+ professional design examples
- Better than default GPT-2, especially if using Mistral-7B

---

## ğŸš€ QUICK START - Run Everything

### 1. Start Services (ML + Backend)
```bash
cd /home/vikas/Desktop/adgenesis
./start_ml_backend.sh
```

This starts:
- ğŸ¤– **ML Service** on `http://localhost:8001` (loads your trained model)
- ğŸ”§ **Backend API** on `http://localhost:8000` (uses ML service for generation)

### 2. Test the Integration
```bash
# In a new terminal
python test_integration.py
```

You should see:
```
âœ… ML Service: healthy
âœ… Backend API: healthy
âœ… ML Service generated design successfully
âœ… Backend generated design successfully
ğŸ‰ All tests passed! Your trained model is working!
```

### 3. Start Frontend
```bash
cd frontend
npm install  # if first time
npm start
```

Open `http://localhost:3000` and create designs! ğŸ‰

---

## ğŸ§ª Manual Testing

### Test ML Service Directly
```bash
cd ml_pipeline
python client.py
```

Expected: JSON blueprint with proper layout, colors, fonts, elements

### Test Through Backend API
```bash
curl -X POST http://localhost:8000/api/design/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Summer sale poster with beach vibes",
    "platform": "instagram",
    "format": "post"
  }'
```

---

## ğŸ“ File Structure

```
adgenesis/
â”œâ”€â”€ ml_pipeline/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ fine_tuned/
â”‚   â”‚       â””â”€â”€ design_model/           â­ Your trained model
â”‚   â”‚           â”œâ”€â”€ config.json
â”‚   â”‚           â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚           â”œâ”€â”€ tokenizer.json
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”œâ”€â”€ serve_design.py                 ğŸ¤– ML service (loads model)
â”‚   â”œâ”€â”€ client.py                       ğŸ§ª Test client
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ design_training_data_formatted.json  ğŸ“Š Training data
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ .env                            âš™ï¸  MODEL_PROVIDER=custom
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ model_adapter.py            ğŸ”„ Routes to ML service
â”‚
â”œâ”€â”€ start_ml_backend.sh                 ğŸš€ Start everything
â””â”€â”€ test_integration.py                 âœ… Test script
```

---

## âš™ï¸ Configuration

### Backend `.env` (already set up)
```env
MODEL_PROVIDER=custom              # Uses your trained model
ML_SERVICE_URL=http://localhost:8001
```

To switch back to OpenAI (if needed):
```env
MODEL_PROVIDER=openai
OPENAI_API_KEY=sk-proj-...
```

---

## ğŸ¯ How It Works Now

1. **User creates design** in frontend
2. **Frontend calls backend** `/api/design/generate`
3. **Backend checks** `MODEL_PROVIDER` in `.env`
4. **Since it's "custom"**, backend calls ML service at `localhost:8001`
5. **ML service loads** your trained model from `design_model/`
6. **Model generates** structured JSON blueprint
7. **Backend returns** design to frontend
8. **Frontend renders** the design visually

---

## ğŸ”§ Troubleshooting

### Services won't start?
```bash
# Check if ports are busy
lsof -i :8000  # Backend
lsof -i :8001  # ML service

# Kill if needed
pkill -f uvicorn
pkill -f serve_design.py
```

### Model not loading?
```bash
# Check model files exist
ls -lh ml_pipeline/models/fine_tuned/design_model/

# Should see:
# - config.json
# - pytorch_model.bin (or model.safetensors)
# - tokenizer files
```

### Low quality outputs?
- Make sure your friend used **Mistral-7B** not GPT-2
- Check training logs - loss should be < 1.0
- May need more training epochs (increase in `train_design_model.py`)

---

## ğŸ“ˆ Improving the Model

### Retrain with More Data
1. Add more examples to `design_training_data_formatted.json`
2. Re-run training on Colab
3. Replace `design_model/` folder
4. Restart services

### Use Better Base Model
In `train_design_model.py`, line 48:
```python
model_name = "mistralai/Mistral-7B-v0.1"  # Better quality
# or
model_name = "meta-llama/Llama-2-7b-hf"   # Alternative
```

---

## ğŸ‰ Success Checklist

- [ ] `./start_ml_backend.sh` runs without errors
- [ ] `python test_integration.py` shows all âœ…
- [ ] Frontend connects and generates designs
- [ ] Designs look professional (good colors, layout, fonts)
- [ ] Backend shows "Using custom model" in logs

---

## ğŸ“ Need Help?

Check logs:
```bash
# ML Service log
tail -f ml_pipeline/serve_design.log

# Backend log (if redirected)
tail -f backend/backend.log
```

Common issues:
- **Out of memory**: Model too large, use 4-bit quantization
- **Slow generation**: Normal on CPU (30s-1min), faster on GPU
- **Bad outputs**: Retrain with more/better data

---

**You're all set!** ğŸš€ The trained model is ready to generate high-quality designs.
